<!doctype html>
<html>
<style>
	.container{
		width: 1200px;
		margin: auto;
		text-align: center;
	
				
	}
	.paper_title{
		font-size: 22px;
		color: green;
		width: 1000px;
		float: left;
		position: relative;
		top: 200px;
	}
	.paper_image{
		float: left;
		position: relative;
		left: 18px;
		top: 250px;	
	}
	.paper_time{
		color: gainsboro;
		width: 300px;
		float: left;
		font-size: 14px;
		position: absolute;
		top: 500px
		
		
		
	}
	.paper_info{
		width: 900px;
		text-align: left;
		position: relative;
		top: 250px;
		left: 70px;
		float:left;
		font-size: 18px;
		line-height: 30px;
		font-size: 15px
	}
 .bottom_info {
    width: 100%;
    height: 150px;
    background-color: white;
	float: left;
	margin-top: 400px;
  }
	#group_info{
		font-size: 15px;

		margin-top: 30px;
		float: left;
		margin-left: 200px;
		
	}
#group_detail_info{
		clear: both;
		font-size: 15px;
		margin-top: 30px;
		float: left;
		margin-left: 200px;

	}
	.image_logo{

		margin-top: 2px;
	}
	.return_index{
		color:green;
		float: left;
		position: relative;
		top: 270px;
		left: 370px;
		
	}
	.return_index_a{
		text-decoration: none;
		color: green;
	}
	.return_index_a:hover{
		text-decoration: underline;
		color: darkgreen;
	}
	.project_leaders{
		 color:green;
		font-size: 22px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 300px;
	
		
	}
	.project_leaders_name{
		color:black;
		font-size: 16px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 320px;
	
	}
	.project_example{
		 color:green;
		font-size: 22px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 500px;
	}
	.project_example_images{
		width: 1200px;
		float: left;
		position: relative;
		top: 500px;
		margin-top: 20px;
		
	}
	.example_text{
		width:1200px;
		text-align: center;
		font-size: 15px;
		line-height: 30px;
		float: left;
		margin-bottom: 150px;
		
		
	}
</style>
<head>
<meta charset="utf-8">
<title>Research</title>
</head>

<body>
		<div class="container">
					<div class="paper_title">Automatic Detection of Breast Lesions in Automated 3D Breast Ultrasound with Cross-Organ Transfer Learning</div>
					<div class="paper_image"><img src="picture1.png" width="300px" ></div>
					<div class="paper_time">Published  22 May 2024</div>
						<div class="paper_info">Breast cancer is a widespread disease that endangers the declaration, effective detection and diagnosis methods are needed to improve the prognosis of patients. At the present stage, most breast cancer screening relies on 2D breast ultrasound, which requires radiologists to imagine the 3D composition of breast lesions. Automated 3D Breast Ultrasound (ABUS) solves this problem very well, and more and more computer-assisted diagnosis (CAD) software developed for ABUS is emerging to reduce the burden on radiologists. However, due to the relatively small popularity of ABUS and the protection of patient privacy by laws and regulations, training CAD software often encounters the problem of small datasets, which limits the software’s performance. 	<p></p>

						We proposed a cross-organ and cross-modality transfer learning method. The detection experience of lung nodules was transferred to the detection of breast lesions and compared with the results transferred from the breast MRI and ABUS datasets. And we also proposed two contrastive learning methods based on BI-RADS grading, which perform contrastive learning at both lesion-level and grade-level. We explored the possibility of using clinical indicators instead of gold standards in deep learning.	<p></p>

						The experimental results on lung nodules, breast Magnetic Resonance Imaging (MRI), and ABUS datasets show that the detection experience of transferring lung nodules in breast cancer detection tasks effectively improves the model’s performance, with average sensitivity increased by up to 4.82%. In the ABUS dataset with a relatively large amount of data, transferring lung nodule detection experience achieved better results compared to transferring breast MRI detection experience, performance improvement increased by 1.03%. The experimental results on the ABUS dataset showed that both methods helped improve model performance, and the contrastive learning method at the lesion-level showed better performance, with a maximum sensitivity improvement of 3.38%.	<p></p>


			
							
						</div>
					<div class="return_index"><a  class="return_index_a" href="../research_info.html">← Back to overview</a></div>
					<div class="project_leaders">Project leaders</div>
					<div class="project_leaders_name">Zhengrui Huang</div>	
					<div class="project_leaders" style="margin-top:50px">Partner Organisations</div>
					<div class="project_leaders_name">杭州市第一人民医院 </div>
				
					<div class="project_leaders_name"></div>
					<div class="project_example">Project Example</div>
					<div class="project_example_images">
						<div class="project_example_images_"><img src="picture1.png" width="700px" ></div>
						<div class="project_example_images_"><img src="picture2.png" width="700px" ></div>

						<div class="example_text">Predict case from our model, the red box represents the possible lesions predicted by our model.</div>

					</div>

			<div class="bottom_info"> 
					  <div  style="width:1200px;height: 150px;margin: auto">
							<div id="group_info"> © Generalized Electric Medicine 2023 of Macao polytechnic university.</div>
							<div id="group_detail_info">The Generalized Electric Medicine is part of the Macao polytechnic university.</div>
							<div class="image_logo"> <img src="../gem_logopng.png"  style="height:140px;right: 0" ></div>
					   </div>
			 </div>
		</div>
</body>
</html>
